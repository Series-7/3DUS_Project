{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BN calc net...\n",
      "Calculate BN stats...\n",
      "progress: 1/453\n",
      "progress: 2/453\n",
      "progress: 3/453\n",
      "progress: 4/453\n",
      "progress: 5/453\n",
      "progress: 6/453\n",
      "progress: 7/453\n",
      "progress: 8/453\n",
      "progress: 9/453\n",
      "progress: 10/453\n",
      "progress: 11/453\n",
      "progress: 12/453\n",
      "progress: 13/453\n",
      "progress: 14/453\n",
      "progress: 15/453\n",
      "progress: 16/453\n",
      "progress: 17/453\n",
      "progress: 18/453\n",
      "progress: 19/453\n",
      "progress: 20/453\n",
      "progress: 21/453\n",
      "progress: 22/453\n",
      "progress: 23/453\n",
      "progress: 24/453\n",
      "progress: 25/453\n",
      "progress: 26/453\n",
      "progress: 27/453\n",
      "progress: 28/453\n",
      "progress: 29/453\n",
      "progress: 30/453\n",
      "progress: 31/453\n",
      "progress: 32/453\n",
      "progress: 33/453\n",
      "progress: 34/453\n",
      "progress: 35/453\n",
      "progress: 36/453\n",
      "progress: 37/453\n",
      "progress: 38/453\n",
      "progress: 39/453\n",
      "progress: 40/453\n",
      "progress: 41/453\n",
      "progress: 42/453\n",
      "progress: 43/453\n",
      "progress: 44/453\n",
      "progress: 45/453\n",
      "progress: 46/453\n",
      "progress: 47/453\n",
      "progress: 48/453\n",
      "progress: 49/453\n",
      "progress: 50/453\n",
      "progress: 51/453\n",
      "progress: 52/453\n",
      "progress: 53/453\n",
      "progress: 54/453\n",
      "progress: 55/453\n",
      "progress: 56/453\n",
      "progress: 57/453\n",
      "progress: 58/453\n",
      "progress: 59/453\n",
      "progress: 60/453\n",
      "progress: 61/453\n",
      "progress: 62/453\n",
      "progress: 63/453\n",
      "progress: 64/453\n",
      "progress: 65/453\n",
      "progress: 66/453\n",
      "progress: 67/453\n",
      "progress: 68/453\n",
      "progress: 69/453\n",
      "progress: 70/453\n",
      "progress: 71/453\n",
      "progress: 72/453\n",
      "progress: 73/453\n",
      "progress: 74/453\n",
      "progress: 75/453\n",
      "progress: 76/453\n",
      "progress: 77/453\n",
      "progress: 78/453\n",
      "progress: 79/453\n",
      "progress: 80/453\n",
      "progress: 81/453\n",
      "progress: 82/453\n",
      "progress: 83/453\n",
      "progress: 84/453\n",
      "progress: 85/453\n",
      "progress: 86/453\n",
      "progress: 87/453\n",
      "progress: 88/453\n",
      "progress: 89/453\n",
      "progress: 90/453\n",
      "progress: 91/453\n",
      "progress: 92/453\n",
      "progress: 93/453\n",
      "progress: 94/453\n",
      "progress: 95/453\n",
      "progress: 96/453\n",
      "progress: 97/453\n",
      "progress: 98/453\n",
      "progress: 99/453\n",
      "progress: 100/453\n",
      "progress: 101/453\n",
      "progress: 102/453\n",
      "progress: 103/453\n",
      "progress: 104/453\n",
      "progress: 105/453\n",
      "progress: 106/453\n",
      "progress: 107/453\n",
      "progress: 108/453\n",
      "progress: 109/453\n",
      "progress: 110/453\n",
      "progress: 111/453\n",
      "progress: 112/453\n",
      "progress: 113/453\n",
      "progress: 114/453\n",
      "progress: 115/453\n",
      "progress: 116/453\n",
      "progress: 117/453\n",
      "progress: 118/453\n",
      "progress: 119/453\n",
      "progress: 120/453\n",
      "progress: 121/453\n",
      "progress: 122/453\n",
      "progress: 123/453\n",
      "progress: 124/453\n",
      "progress: 125/453\n",
      "progress: 126/453\n",
      "progress: 127/453\n",
      "progress: 128/453\n",
      "progress: 129/453\n",
      "progress: 130/453\n",
      "progress: 131/453\n",
      "progress: 132/453\n",
      "progress: 133/453\n",
      "progress: 134/453\n",
      "progress: 135/453\n",
      "progress: 136/453\n",
      "progress: 137/453\n",
      "progress: 138/453\n",
      "progress: 139/453\n",
      "progress: 140/453\n",
      "progress: 141/453\n",
      "progress: 142/453\n",
      "progress: 143/453\n",
      "progress: 144/453\n",
      "progress: 145/453\n",
      "progress: 146/453\n",
      "progress: 147/453\n",
      "progress: 148/453\n",
      "progress: 149/453\n",
      "progress: 150/453\n",
      "progress: 151/453\n",
      "progress: 152/453\n",
      "progress: 153/453\n",
      "progress: 154/453\n",
      "progress: 155/453\n",
      "progress: 156/453\n",
      "progress: 157/453\n",
      "progress: 158/453\n",
      "progress: 159/453\n",
      "progress: 160/453\n",
      "progress: 161/453\n",
      "progress: 162/453\n",
      "progress: 163/453\n",
      "progress: 164/453\n",
      "progress: 165/453\n",
      "progress: 166/453\n",
      "progress: 167/453\n",
      "progress: 168/453\n",
      "progress: 169/453\n",
      "progress: 170/453\n",
      "progress: 171/453\n",
      "progress: 172/453\n",
      "progress: 173/453\n",
      "progress: 174/453\n",
      "progress: 175/453\n",
      "progress: 176/453\n",
      "progress: 177/453\n",
      "progress: 178/453\n",
      "progress: 179/453\n",
      "progress: 180/453\n",
      "progress: 181/453\n",
      "progress: 182/453\n",
      "progress: 183/453\n",
      "progress: 184/453\n",
      "progress: 185/453\n",
      "progress: 186/453\n",
      "progress: 187/453\n",
      "progress: 188/453\n",
      "progress: 189/453\n",
      "progress: 190/453\n",
      "progress: 191/453\n",
      "progress: 192/453\n",
      "progress: 193/453\n",
      "progress: 194/453\n",
      "progress: 195/453\n",
      "progress: 196/453\n",
      "progress: 197/453\n",
      "progress: 198/453\n",
      "progress: 199/453\n",
      "progress: 200/453\n",
      "progress: 201/453\n",
      "progress: 202/453\n",
      "progress: 203/453\n",
      "progress: 204/453\n",
      "progress: 205/453\n",
      "progress: 206/453\n",
      "progress: 207/453\n",
      "progress: 208/453\n",
      "progress: 209/453\n",
      "progress: 210/453\n",
      "progress: 211/453\n",
      "progress: 212/453\n",
      "progress: 213/453\n",
      "progress: 214/453\n",
      "progress: 215/453\n",
      "progress: 216/453\n",
      "progress: 217/453\n",
      "progress: 218/453\n",
      "progress: 219/453\n",
      "progress: 220/453\n",
      "progress: 221/453\n",
      "progress: 222/453\n",
      "progress: 223/453\n",
      "progress: 224/453\n",
      "progress: 225/453\n",
      "progress: 226/453\n",
      "progress: 227/453\n",
      "progress: 228/453\n",
      "progress: 229/453\n",
      "progress: 230/453\n",
      "progress: 231/453\n",
      "progress: 232/453\n",
      "progress: 233/453\n",
      "progress: 234/453\n",
      "progress: 235/453\n",
      "progress: 236/453\n",
      "progress: 237/453\n",
      "progress: 238/453\n",
      "progress: 239/453\n",
      "progress: 240/453\n",
      "progress: 241/453\n",
      "progress: 242/453\n",
      "progress: 243/453\n",
      "progress: 244/453\n",
      "progress: 245/453\n",
      "progress: 246/453\n",
      "progress: 247/453\n",
      "progress: 248/453\n",
      "progress: 249/453\n",
      "progress: 250/453\n",
      "progress: 251/453\n",
      "progress: 252/453\n",
      "progress: 253/453\n",
      "progress: 254/453\n",
      "progress: 255/453\n",
      "progress: 256/453\n",
      "progress: 257/453\n",
      "progress: 258/453\n",
      "progress: 259/453\n",
      "progress: 260/453\n",
      "progress: 261/453\n",
      "progress: 262/453\n",
      "progress: 263/453\n",
      "progress: 264/453\n",
      "progress: 265/453\n",
      "progress: 266/453\n",
      "progress: 267/453\n",
      "progress: 268/453\n",
      "progress: 269/453\n",
      "progress: 270/453\n",
      "progress: 271/453\n",
      "progress: 272/453\n",
      "progress: 273/453\n",
      "progress: 274/453\n",
      "progress: 275/453\n",
      "progress: 276/453\n",
      "progress: 277/453\n",
      "progress: 278/453\n",
      "progress: 279/453\n",
      "progress: 280/453\n",
      "progress: 281/453\n",
      "progress: 282/453\n",
      "progress: 283/453\n",
      "progress: 284/453\n",
      "progress: 285/453\n",
      "progress: 286/453\n",
      "progress: 287/453\n",
      "progress: 288/453\n",
      "progress: 289/453\n",
      "progress: 290/453\n",
      "progress: 291/453\n",
      "progress: 292/453\n",
      "progress: 293/453\n",
      "progress: 294/453\n",
      "progress: 295/453\n",
      "progress: 296/453\n",
      "progress: 297/453\n",
      "progress: 298/453\n",
      "progress: 299/453\n",
      "progress: 300/453\n",
      "progress: 301/453\n",
      "progress: 302/453\n",
      "progress: 303/453\n",
      "progress: 304/453\n",
      "progress: 305/453\n",
      "progress: 306/453\n",
      "progress: 307/453\n",
      "progress: 308/453\n",
      "progress: 309/453\n",
      "progress: 310/453\n",
      "progress: 311/453\n",
      "progress: 312/453\n",
      "progress: 313/453\n",
      "progress: 314/453\n",
      "progress: 315/453\n",
      "progress: 316/453\n",
      "progress: 317/453\n",
      "progress: 318/453\n",
      "progress: 319/453\n",
      "progress: 320/453\n",
      "progress: 321/453\n",
      "progress: 322/453\n",
      "progress: 323/453\n",
      "progress: 324/453\n",
      "progress: 325/453\n",
      "progress: 326/453\n",
      "progress: 327/453\n",
      "progress: 328/453\n",
      "progress: 329/453\n",
      "progress: 330/453\n",
      "progress: 331/453\n",
      "progress: 332/453\n",
      "progress: 333/453\n",
      "progress: 334/453\n",
      "progress: 335/453\n",
      "progress: 336/453\n",
      "progress: 337/453\n",
      "progress: 338/453\n",
      "progress: 339/453\n",
      "progress: 340/453\n",
      "progress: 341/453\n",
      "progress: 342/453\n",
      "progress: 343/453\n",
      "progress: 344/453\n",
      "progress: 345/453\n",
      "progress: 346/453\n",
      "progress: 347/453\n",
      "progress: 348/453\n",
      "progress: 349/453\n",
      "progress: 350/453\n",
      "progress: 351/453\n",
      "progress: 352/453\n",
      "progress: 353/453\n",
      "progress: 354/453\n",
      "progress: 355/453\n",
      "progress: 356/453\n",
      "progress: 357/453\n",
      "progress: 358/453\n",
      "progress: 359/453\n",
      "progress: 360/453\n",
      "progress: 361/453\n",
      "progress: 362/453\n",
      "progress: 363/453\n",
      "progress: 364/453\n",
      "progress: 365/453\n",
      "progress: 366/453\n",
      "progress: 367/453\n",
      "progress: 368/453\n",
      "progress: 369/453\n",
      "progress: 370/453\n",
      "progress: 371/453\n",
      "progress: 372/453\n",
      "progress: 373/453\n",
      "progress: 374/453\n",
      "progress: 375/453\n",
      "progress: 376/453\n",
      "progress: 377/453\n",
      "progress: 378/453\n",
      "progress: 379/453\n",
      "progress: 380/453\n",
      "progress: 381/453\n",
      "progress: 382/453\n",
      "progress: 383/453\n",
      "progress: 384/453\n",
      "progress: 385/453\n",
      "progress: 386/453\n",
      "progress: 387/453\n",
      "progress: 388/453\n",
      "progress: 389/453\n",
      "progress: 390/453\n",
      "progress: 391/453\n",
      "progress: 392/453\n",
      "progress: 393/453\n",
      "progress: 394/453\n",
      "progress: 395/453\n",
      "progress: 396/453\n",
      "progress: 397/453\n",
      "progress: 398/453\n",
      "progress: 399/453\n",
      "progress: 400/453\n",
      "progress: 401/453\n",
      "progress: 402/453\n",
      "progress: 403/453\n",
      "progress: 404/453\n",
      "progress: 405/453\n",
      "progress: 406/453\n",
      "progress: 407/453\n",
      "progress: 408/453\n",
      "progress: 409/453\n",
      "progress: 410/453\n",
      "progress: 411/453\n",
      "progress: 412/453\n",
      "progress: 413/453\n",
      "progress: 414/453\n",
      "progress: 415/453\n",
      "progress: 416/453\n",
      "progress: 417/453\n",
      "progress: 418/453\n",
      "progress: 419/453\n",
      "progress: 420/453\n",
      "progress: 421/453\n",
      "progress: 422/453\n",
      "progress: 423/453\n",
      "progress: 424/453\n",
      "progress: 425/453\n",
      "progress: 426/453\n",
      "progress: 427/453\n",
      "progress: 428/453\n",
      "progress: 429/453\n",
      "progress: 430/453\n",
      "progress: 431/453\n",
      "progress: 432/453\n",
      "progress: 433/453\n",
      "progress: 434/453\n",
      "progress: 435/453\n",
      "progress: 436/453\n",
      "progress: 437/453\n",
      "progress: 438/453\n",
      "progress: 439/453\n",
      "progress: 440/453\n",
      "progress: 441/453\n",
      "progress: 442/453\n",
      "progress: 443/453\n",
      "progress: 444/453\n",
      "progress: 445/453\n",
      "progress: 446/453\n",
      "progress: 447/453\n",
      "progress: 448/453\n",
      "progress: 449/453\n",
      "progress: 450/453\n",
      "progress: 451/453\n",
      "progress: 452/453\n",
      "New data:\n",
      "[u'conv4_2_bn', u'conv2_2_D_bn', u'conv5_2_D_bn', u'conv4_3_bn', u'conv5_3_bn', u'conv1_2_bn', u'conv3_3_bn', u'conv4_1_D_bn', u'conv3_3_D_bn', u'conv3_1_D_bn', u'conv3_1_bn', u'conv5_1_D_bn', u'conv3_2_D_bn', u'conv3_2_bn', u'conv5_3_D_bn', u'conv2_2_bn', u'conv2_1_D_bn', u'conv5_1_bn', u'conv4_1_bn', u'conv4_2_D_bn', u'conv4_3_D_bn', u'conv2_1_bn', u'conv1_2_D_bn', u'conv5_2_bn']\n",
      "[u'conv4_2_bn', u'conv2_2_D_bn', u'conv5_2_D_bn', u'conv4_3_bn', u'conv5_3_bn', u'conv1_2_bn', u'conv3_3_bn', u'conv4_1_D_bn', u'conv3_3_D_bn', u'conv3_1_D_bn', u'conv3_1_bn', u'conv5_1_D_bn', u'conv3_2_D_bn', u'conv3_2_bn', u'conv5_3_D_bn', u'conv2_2_bn', u'conv2_1_D_bn', u'conv5_1_bn', u'conv4_1_bn', u'conv4_2_D_bn', u'conv4_3_D_bn', u'conv2_1_bn', u'conv1_2_D_bn', u'conv5_2_bn']\n",
      "Saving test net weights...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import ImageCollection\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "caffe_root = '/home/anojan/workspace/caffe_install/caffe-segnet-cudnn5/'\n",
    "sys.path.insert(0,'/home/anojan/anaconda2/envs/env/lib/python2.7/site-packages/')\n",
    "sys.path.insert(0,caffe_root + 'python')\n",
    "\n",
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "\n",
    "def extract_dataset(net_message):\n",
    "    source_lab='../us_data/labels.txt'\n",
    "    source_img='../us_data/images.txt'\n",
    "    with open(source_lab) as f:\n",
    "        data = f.read().split()\n",
    "    with open(source_img) as f:\n",
    "        image = f.read().split()\n",
    "    ims = ImageCollection(image[::1])\n",
    "    with open(source_lab) as f:\n",
    "        label = f.read().split()\n",
    "    labs = ImageCollection(label[::1])\n",
    "    assert len(ims) == len(labs) \n",
    "    return ims, labs\n",
    "\n",
    "\n",
    "def make_testable(train_model_path):\n",
    "    # load the train net prototxt as a protobuf message\n",
    "    with open(train_model_path) as f:\n",
    "        train_str = f.read()\n",
    "    train_net = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(train_str, train_net)\n",
    "\n",
    "    # add the mean, var top blobs to all BN layers\n",
    "    for layer in train_net.layer:\n",
    "        if layer.type == \"BN\" and len(layer.top) == 1:\n",
    "            layer.top.append(layer.top[0] + \"-mean\")\n",
    "            layer.top.append(layer.top[0] + \"-var\")\n",
    "\n",
    "    # remove the test data layer if present\n",
    "    if train_net.layer[1].name == \"data\" and train_net.layer[1].include:\n",
    "        train_net.layer.remove(train_net.layer[1])\n",
    "        if train_net.layer[0].include:\n",
    "            # remove the 'include {phase: TRAIN}' layer param\n",
    "            train_net.layer[0].include.remove(train_net.layer[0].include[0])\n",
    "    return train_net\n",
    "\n",
    "\n",
    "def make_test_files(testable_net_path, train_weights_path, num_iterations,\n",
    "                    in_h, in_w):\n",
    "    # load the train net prototxt as a protobuf message\n",
    "    with open(testable_net_path) as f:\n",
    "        testable_str = f.read()\n",
    "    testable_msg = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(testable_str, testable_msg)\n",
    "    \n",
    "    bn_layers = [l.name for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_blobs = [l.top[0] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_means = [l.top[1] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_vars = [l.top[2] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "\n",
    "    net = caffe.Net(testable_net_path, train_weights_path, caffe.TEST)\n",
    "    \n",
    "    # init our blob stores with the first forward pass\n",
    "    res = net.forward()\n",
    "    bn_avg_mean = {bn_mean: np.squeeze(res[bn_mean]).copy() for bn_mean in bn_means}\n",
    "    bn_avg_var = {bn_var: np.squeeze(res[bn_var]).copy() for bn_var in bn_vars}\n",
    "\n",
    "    # iterate over the rest of the training set\n",
    "    for i in xrange(1, num_iterations):\n",
    "        res = net.forward()\n",
    "        for bn_mean in bn_means:\n",
    "            bn_avg_mean[bn_mean] += np.squeeze(res[bn_mean])\n",
    "        for bn_var in bn_vars:\n",
    "            bn_avg_var[bn_var] += np.squeeze(res[bn_var])\n",
    "        print 'progress: {}/{}'.format(i, num_iterations)\n",
    "\n",
    "    # compute average means and vars\n",
    "    for bn_mean in bn_means:\n",
    "        bn_avg_mean[bn_mean] /= num_iterations\n",
    "    for bn_var in bn_vars:\n",
    "        bn_avg_var[bn_var] /= num_iterations\n",
    "\n",
    "    for bn_blob, bn_var in zip(bn_blobs, bn_vars):\n",
    "        m = np.prod(net.blobs[bn_blob].data.shape) / np.prod(bn_avg_var[bn_var].shape)\n",
    "        bn_avg_var[bn_var] *= (m / (m - 1))\n",
    "\n",
    "    # calculate the new scale and shift blobs for all the BN layers\n",
    "    scale_data = {bn_layer: np.squeeze(net.params[bn_layer][0].data)\n",
    "                  for bn_layer in bn_layers}\n",
    "    shift_data = {bn_layer: np.squeeze(net.params[bn_layer][1].data)\n",
    "                  for bn_layer in bn_layers}\n",
    "\n",
    "    var_eps = 1e-9\n",
    "    new_scale_data = {}\n",
    "    new_shift_data = {}\n",
    "    for bn_layer, bn_mean, bn_var in zip(bn_layers, bn_means, bn_vars):\n",
    "        gamma = scale_data[bn_layer]\n",
    "        beta = shift_data[bn_layer]\n",
    "        Ex = bn_avg_mean[bn_mean]\n",
    "        Varx = bn_avg_var[bn_var]\n",
    "        new_gamma = gamma / np.sqrt(Varx + var_eps)\n",
    "        new_beta = beta - (gamma * Ex / np.sqrt(Varx + var_eps))\n",
    "\n",
    "        new_scale_data[bn_layer] = new_gamma\n",
    "        new_shift_data[bn_layer] = new_beta\n",
    "    print \"New data:\"\n",
    "    print new_scale_data.keys()\n",
    "    print new_shift_data.keys()\n",
    "\n",
    "    # assign computed new scale and shift values to net.params\n",
    "    for bn_layer in bn_layers:\n",
    "        net.params[bn_layer][0].data[...] = new_scale_data[bn_layer].reshape(\n",
    "            net.params[bn_layer][0].data.shape\n",
    "        )\n",
    "        net.params[bn_layer][1].data[...] = new_shift_data[bn_layer].reshape(\n",
    "            net.params[bn_layer][1].data.shape\n",
    "        )\n",
    "        \n",
    "    # build a test net prototxt\n",
    "    test_msg = testable_msg\n",
    "    # replace data layers with 'input' net param\n",
    "    data_layers = [l for l in test_msg.layer if l.type.endswith(\"Data\")]\n",
    "    for data_layer in data_layers:\n",
    "        test_msg.layer.remove(data_layer)\n",
    "    test_msg.input.append(\"data\")\n",
    "    test_msg.input_dim.append(1)\n",
    "    test_msg.input_dim.append(3)\n",
    "    test_msg.input_dim.append(in_h)\n",
    "    test_msg.input_dim.append(in_w)\n",
    "    # Set BN layers to INFERENCE so they use the new stat blobs\n",
    "    # and remove mean, var top blobs.\n",
    "    for l in test_msg.layer:\n",
    "        if l.type == \"BN\":\n",
    "            if len(l.top) > 1:\n",
    "                dead_tops = l.top[1:]\n",
    "                for dl in dead_tops:\n",
    "                    l.top.remove(dl)\n",
    "            l.bn_param.bn_mode = caffe_pb2.BNParameter.INFERENCE\n",
    "    # replace output loss, accuracy layers with a softmax\n",
    "    dead_outputs = [l for l in test_msg.layer if l.type in [\"SoftmaxWithLoss\", \"Accuracy\"]]\n",
    "    out_bottom = dead_outputs[0].bottom[0]\n",
    "    for dead in dead_outputs:\n",
    "        test_msg.layer.remove(dead)\n",
    "    test_msg.layer.add(\n",
    "        name=\"prob\", type=\"Softmax\", bottom=[out_bottom], top=['prob']\n",
    "    )\n",
    "    return net, test_msg\n",
    "\n",
    "\n",
    "def make_parser():\n",
    "    p = ArgumentParser()\n",
    "    p.add_argument('train_model')\n",
    "    p.add_argument('weights')\n",
    "    p.add_argument('out_dir')\n",
    "    return p\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    caffe.set_mode_gpu()\n",
    "    p = make_parser()\n",
    "    \n",
    "    \n",
    "    out_dir = '/home/anojan/us_project/us_data/';\n",
    "    train_model = '/home/anojan/us_project/models/segnet_train.prototxt';\n",
    "    weights = '/home/anojan/us_project/caffemodels_iter_9000.caffemodel';\n",
    "\n",
    "    # build and save testable net\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    print \"Building BN calc net...\"\n",
    "    testable_msg = make_testable(train_model)\n",
    "    BN_calc_path = os.path.join(\n",
    "        out_dir, '__for_calculating_BN_stats_' + os.path.basename(train_model)\n",
    "    )\n",
    "    with open(BN_calc_path, 'w') as f:\n",
    "        f.write(text_format.MessageToString(testable_msg))\n",
    "\n",
    "    # use testable net to calculate BN layer stats\n",
    "    print \"Calculate BN stats...\"\n",
    "    train_size,image_shape = extract_dataset(testable_msg)\n",
    "    \n",
    "    \n",
    "    minibatch_size = 1 #testable_msg.layer[0].dense_image_data_param.batch_size\n",
    "    num_iterations = 453\n",
    "    in_h, in_w =(560, 360)\n",
    "    test_net, test_msg = make_test_files(BN_calc_path, weights, num_iterations,\n",
    "                                         in_h, in_w)\n",
    "    \n",
    "    # save deploy prototxt\n",
    "    #print \"Saving deployment prototext file...\"\n",
    "    #test_path = os.path.join(args.out_dir, \"deploy.prototxt\")\n",
    "    #with open(test_path, 'w') as f:\n",
    "    #    f.write(text_format.MessageToString(test_msg))\n",
    "    \n",
    "    print \"Saving test net weights...\"\n",
    "    test_net.save(os.path.join(out_dir, \"test_weights_iter_9000.caffemodel\"))\n",
    "    print \"done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<skimage.io.collection.ImageCollection at 0x7fa5abf364d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
